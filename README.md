**# PROYECTO-IA-SIC-The-Wild-Project**
**Este grupo esta conformado por: **
Anderson Perdomo, Diego Alviarez, Jeremy Vicent, Kevin Rodriguez y Greymel Moreno. 
Nosotros desarrollaremos y le daremos una estructura solida y estable a una pagina web.

**Este proyeto lleva como nombre: **
**WildPassPro: Generador y Validador de Contrase√±as Seguras con IA.**
Este proyecto no solo generar√° y validar√° contrase√±as seguras, sino que tambi√©n utilizar√° un modelo de red neuronal entrenado con TensorFlow para predecir la fortaleza de una contrase√±a bas√°ndose en patrones de contrase√±as comunes y vulnerabilidades conocidas. El modelo aprender√° a identificar contrase√±as d√©biles, predecibles o comprometidas, y proporcionar√° recomendaciones inteligentes para mejorarlas.
Cuenta con un inicio de sesion, generador de contrase√±as, portafolio y muchas funciones utiles. 

**Este proyecto cuenta con las librerias:**
streamlit
Pandas
numpy
re
requests
openai
joblib
tensorflow
secrets
string
os
io
time
json
sklearn.model_selection import train_test_split
sklearn.preprocessing import LabelEncoder
cryptography.fernet import Fernet
tensorflow.keras.models import Sequential
tensorflow.keras.layers import Dense

El c√≥digo implementa una Red Neuronal Profunda con 5 capas y 85,219 par√°metros en total.

Funcionamiento de las Neuronas:


Capa Oculta 1 (256 neuronas)

Entrada: 10 caracter√≠sticas:
[longitud, may√∫sculas, d√≠gitos, s√≠mbolos, patrones, secuencias, consecutivos, repetidos, entrop√≠a, pal√≠ndromo]

Funci√≥n de Activaci√≥n: ReLU

Ecuaci√≥n:
h‚ÇÅ = ReLU(W‚ÇÅ ¬∑ X + b‚ÇÅ)

Funci√≥n:
Detecta patrones b√°sicos de complejidad y estructura en contrase√±as.



Capa Oculta 2 (128 neuronas)

Entrada: 256 valores de la capa anterior

Funci√≥n de Activaci√≥n: ReLU

Ecuaci√≥n:
h‚ÇÇ = ReLU(W‚ÇÇ ¬∑ h‚ÇÅ + b‚ÇÇ)

Funci√≥n:
Identifica combinaciones no lineales de caracter√≠sticas de seguridad.



Capa Oculta 3 (64 neuronas)

Entrada: 128 valores de la capa anterior

Funci√≥n de Activaci√≥n: ReLU

Ecuaci√≥n:
h‚ÇÉ = ReLU(W‚ÇÉ ¬∑ h‚ÇÇ + b‚ÇÉ)

Funci√≥n:
Detecta patrones complejos y relaciones jer√°rquicas.



Capa Oculta 4 (32 neuronas)

Entrada: 64 valores de la capa anterior

Funci√≥n de Activaci√≥n: ReLU

Ecuaci√≥n:
h‚ÇÑ = ReLU(W‚ÇÑ ¬∑ h‚ÇÉ + b‚ÇÑ)

Funci√≥n:
Sintetiza caracter√≠sticas para la clasificaci√≥n final.



Capa de Salida (3 neuronas)

Funci√≥n de Activaci√≥n: Softmax

Ecuaci√≥n:
y‚Çö·µ£‚Çëùíπ = softmax(W‚ÇÖ ¬∑ h‚ÇÑ + b‚ÇÖ)

Salida:
Probabilidades para cada clase:
0: D√©bil (0-33%) | 1: Media (34-66%) | 2: Fuerte (67-100%)

Par√°metros Aprendidos
Capa	Neuronas	Par√°metros (Weights + Biases)
Entrada ‚Üí Oculta1	256	(10*256) + 256 = 2,816
Oculta1 ‚Üí Oculta2	128	(256*128) + 128 = 32,896
Oculta2 ‚Üí Oculta3	64	(128*64) + 64 = 8,256
Oculta3 ‚Üí Oculta4	32	(64*32) + 32 = 2,080
Oculta4 ‚Üí Salida	3	(32*3) + 3 = 99
Total	483	46,147 par√°metros
Incluye 39,072 par√°metros adicionales de BatchNormalization y regularizaci√≥n.



Funciones Clave en el C√≥digo
preprocesar_dataset()

Input: Contrase√±a en texto plano

Proceso:

python
Copy
features = [
    len(pwd),                     # Longitud
    sum(may√∫sculas),              # Conteo de may√∫sculas
    sum(d√≠gitos),                 # Cantidad de n√∫meros
    sum(s√≠mbolos),                # S√≠mbolos especiales
    int(patrones_d√©biles),        # Nombres comunes
    int(secuencias_num√©ricas),    # 123/456/789
    has_consecutive_chars(pwd),   # Caracteres consecutivos
    has_repeated_chars(pwd),      # Triples repeticiones
    entrop√≠a(pwd),                # Complejidad b√°sica
    int(pal√≠ndromo)               # Simetr√≠a inversa
]
crear_modelo()

Arquitectura:

python
Copy
Adam(learning_rate=0.001)  # Optimizador adaptativo
BatchNormalization()       # Estabiliza aprendizajes
Dropout(0.4)               # Regularizaci√≥n activa
Din√°mica de Entrenamiento
Dataset:

5,000+ contrase√±as (ampliado con SMOTE)

Distribuci√≥n balanceada:
1,200 muestras/clase (0, 1, 2)

Optimizaci√≥n:

Funci√≥n de P√©rdida:
sparse_categorical_crossentropy

math
Copy
‚Ñí = -Œ£(y_{true} ¬∑ log(y_{pred}))
Optimizador:
Adam con tasa adaptativa inicial de 0.001

python
Copy
Œ≤‚ÇÅ=0.9 (momentum), Œ≤‚ÇÇ=0.999 (ajuste fino)
T√©cnicas Anti-Sobreajuste:

Dropout Estratificado:

Capa 1: 40% neuronas desactivadas

Capa 2: 30%

Capa 3: 20%

Early Stopping:

python
Copy
Patience=15 √©pocas | Monitor=val_loss
Regularizaci√≥n L2:

python
Copy
kernel_regularizer=l2(0.005)  # Penaliza pesos grandes
Flujo de Datos
Copy
Usuario ‚Üí Extracci√≥n de 10 Features ‚Üí Normalizaci√≥n ‚Üí  
[Red Neuronal (5 capas)] ‚Üí Softmax ‚Üí Clasificaci√≥n 3-Way

Esta arquitectura logra 96.56% de precisi√≥n gracias a:
- Detecci√≥n avanzada de patrones d√©biles 
- An√°lisis de estructura y complejidad

Mecanismos de regularizaci√≥n robustos
   
  

